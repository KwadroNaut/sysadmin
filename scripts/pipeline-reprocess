#!/usr/bin/make -f

########################################################################
# Settings:
#

# all flag files are created in separate directory
# To run new tasks you have to mind dependencies, e.g. you may have to mark some TIs as successful.
PRJ = spill
# sed-deps, af-deps, or metadb-deps -- something like that
TYPEOF_DEPS = sed-deps

AF_TASK = meta_pg
AF_ARGS = --force # Use `--force` to ignore current TI state. It's not needed for new tasks, but it's useful for re-processing tasks like `meta_pg`.

########################################################################
# Makefile targets & rules
#

.SILENT: # to avoid useless echo before `ONESHELL` invocation
.ONESHELL: # to use here-docs
.SHELLFLAGS = -o pipefail -xec # xtrace, errexit, command
SHELL = /bin/bash
MAKEFLAGS = --jobs 4 --load-average 8 --keep-going # datacollector.infra.ooni.io specs

.PHONY: help all rc pause reprocess

ifndef TMUX
    $(error Run this makefile under TMUX)
endif

THIS := $(word $(words $(MAKEFILE_LIST)),$(MAKEFILE_LIST))

help:
	set +x
	echo "Usage:"
	echo "  $${EDITOR:-vi} $(THIS) -- to tune options"
	echo "  make reprocess -- start spawning TaskInstances"
	echo "  make pause -- grab a lock to prevent new TaskInstances from spawning"
	echo "  make rc -- list exit() status for TaskInstances having flag in ./$(PRJ)/"

rc:
	cd $(PRJ) && sudo grep -F 'Task exited with return code' $$(ls *.tiflag | sed 's,^,/srv/af-worker/logs/hist_canning/$(AF_TASK)/,; s,.tiflag$$,T00:00:00,') | awk -F: '(key && key != $$1) {print acc} {key = $$1; acc = $$0;} END {print acc}'

-include $(PRJ)/sed-deps
all: reprocess

pause: $(PRJ)/.keep
	flock $(PRJ)/.keep sh -c 'while :; do date; ps -C flock -o pid,tty,user,lstart,command; sleep 5; done'

$(PRJ)/.keep :
	mkdir -p $(PRJ)
	touch $@

$(PRJ)/sed-deps : affected.bucket_date $(PRJ)/.keep
	sed 's,^20[0-9][0-9]-[01][0-9]-[0-3][0-9]$$,reprocess : $(PRJ)/&.tiflag,' <affected.bucket_date >$@.tmp
	mv $@.tmp $@

$(PRJ)/af-deps : $(PRJ)/.keep
	set +o xtrace
	. <(sudo docker inspect af-psql | jq -r '.[].Config.Env[] | select(startswith("POSTGRES_"))' | sed 's/^POSTGRES_/PG/')
	set -o xtrace
	export PGUSER PGPASSWORD
	export PGHOSTADDR=`sudo docker inspect af-psql | jq -r '.[].NetworkSettings.Networks.af.IPAddress'`
	psql airflow --no-align --tuples-only '--set=prj=$(PRJ)' >$@.tmp <<EOF
	SELECT 'reprocess : ' || :'prj' || '/' || execution_date::date || '.tiflag'
	FROM task_instance
	WHERE dag_id = 'hist_canning' AND task_id = 'canning'
	EOF
	mv $@.tmp $@

$(PRJ)/metadb-deps : $(PRJ)/.keep
	set +o xtrace
	. <(sudo cat /srv/etc/af-worker/hkgmetadb.env)
	set -o xtrace
	export PGPASSWORD
	psql --host=hkgmetadb.infra.ooni.io --username shovel --dbname metadb --no-align --tuples-only '--set=prj=$(PRJ)' >$@.tmp <<EOF
	SELECT 'reprocess : ' || :'prj' || '/' || bucket_date || '.tiflag'
	FROM autoclaved
	GROUP BY bucket_date
	HAVING MIN(code_ver) < 5
	ORDER BY bucket_date DESC
	EOF
	mv $@.tmp $@

$(PRJ)/%.tiflag: $(PRJ)/.keep
	set +o xtrace
	. <(sudo docker inspect af-psql | jq -r '.[].Config.Env[] | select(startswith("POSTGRES_"))' | sed 's/^POSTGRES_/PG/')
	set -o xtrace
	export PGUSER PGPASSWORD
	export PGHOSTADDR=`sudo docker inspect af-psql | jq -r '.[].NetworkSettings.Networks.af.IPAddress'`
	start=`psql --dbname airflow --no-align --tuples-only -c 'select CURRENT_TIMESTAMP'`
	# flock is here to avoid concurrent start
	flock $(PRJ)/.keep sudo docker exec -i af-websrv airflow run $(AF_ARGS) hist_canning $(AF_TASK) '$*T00:00:00'
	while [ `psql --dbname airflow --no-align --tuples-only "--set=start=$$start" '--set=ex=$*T00:00:00' <<EOF
	SELECT COUNT(*) FROM task_instance
	WHERE dag_id = 'hist_canning' AND task_id = '$(AF_TASK)' AND execution_date = :'ex' AND :'start' < end_date
	EOF` != 1 ]; do sleep 30; done
	date >$@
